# FineTune-DistilGPT2-CausalLM
This project fine-tunes DistilGPT2 for causal language modeling using LoRA and BitsAndBytes quantization. It processes ELI5 data, tokenizes text, and trains an efficient model for text generation. The model is optimized for low-memory training and generates coherent responses.
